{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Connect to Spark Connect server\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkConnectTest\") \\\n",
    "    .remote(\"sc://192.168.1.26:15002\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Test with a simple DataFrame\n",
    "df = spark.range(start=0, end=1000000, step=10, numPartitions= 4).repartition(4)\n",
    "result = df.count()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31879dad",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_MultiThreadedRendezvous\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/client/core.py:1523\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req, observations, progress)\u001b[39m\n\u001b[32m   1520\u001b[39m generator = ExecutePlanResponseReattachableIterator(\n\u001b[32m   1521\u001b[39m     req, \u001b[38;5;28mself\u001b[39m._stub, \u001b[38;5;28mself\u001b[39m._retrying, \u001b[38;5;28mself\u001b[39m._builder.metadata()\n\u001b[32m   1522\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1523\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhandle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen _collections_abc>:360\u001b[39m, in \u001b[36m__next__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/client/reattach.py:138\u001b[39m, in \u001b[36mExecutePlanResponseReattachableIterator.send\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: Any) -> pb2.ExecutePlanResponse:\n\u001b[32m    137\u001b[39m     \u001b[38;5;66;03m# will trigger reattach in case the stream completed without result_complete\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_has_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    139\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/client/reattach.py:190\u001b[39m, in \u001b[36mExecutePlanResponseReattachableIterator._has_next\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    189\u001b[39m     \u001b[38;5;28mself\u001b[39m._release_all()\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/client/reattach.py:162\u001b[39m, in \u001b[36mExecutePlanResponseReattachableIterator._has_next\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28mself\u001b[39m._current = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/client/reattach.py:281\u001b[39m, in \u001b[36mExecutePlanResponseReattachableIterator._call_iter\u001b[39m\u001b[34m(self, iter_fun)\u001b[39m\n\u001b[32m    280\u001b[39m         \u001b[38;5;28mself\u001b[39m._iterator = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    283\u001b[39m     \u001b[38;5;66;03m# Remove the iterator, so that a new one will be created after retry.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/client/reattach.py:261\u001b[39m, in \u001b[36mExecutePlanResponseReattachableIterator._call_iter\u001b[39m\u001b[34m(self, iter_fun)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miter_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/client/reattach.py:163\u001b[39m, in \u001b[36mExecutePlanResponseReattachableIterator._has_next.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;28mself\u001b[39m._current = \u001b[38;5;28mself\u001b[39m._call_iter(\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/grpc/_channel.py:543\u001b[39m, in \u001b[36m_Rendezvous.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/grpc/_channel.py:969\u001b[39m, in \u001b[36m_MultiThreadedRendezvous._next\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state.code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[31m_MultiThreadedRendezvous\u001b[39m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.sql.connect.service.SparkConnectServer.main(SparkConnectServer.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\njava.base/java.lang.reflect.Method.invoke(Unknown Source)\norg.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\norg.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1027)\norg.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\norg.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\norg.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\norg.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n\nAnd it was stopped at:\n\norg.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:184)\njava.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\njava.base/java.util.concurrent.FutureTask.run(Unknown Source)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\njava.base/java.lang.Thread.run(Unknown Source)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.sql.connect.service.SparkConnectServer.main(SparkConnectServer.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\njava.base/java.lang.reflect.Method.invoke(Unknown Source)\norg.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\norg.apache...\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:192.168.1.26:15002 {grpc_status:13, grpc_message:\"Cannot call methods on a stopped SparkContext.\\nThis stopped SparkContext was created at:\\n\\norg.apache.spark.sql.connect.service.SparkConnectServer.main(SparkConnectServer.scala)\\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\\njava.base/java.lang.reflect.Method.invoke(Unknown Source)\\norg.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\\norg.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1027)\\norg.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\\norg.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\\norg.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\\norg.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)\\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)\\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\\n\\nAnd it was stopped at:\\n\\norg.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:184)\\njava.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\\njava.base/java.util.concurrent.FutureTask.run(Unknown Source)\\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\njava.base/java.lang.Thread.run(Unknown Source)\\n\\nThe currently active SparkContext was created at:\\n\\norg.apache.spark.sql.connect.service.SparkConnectServer.main(SparkConnectServer.scala)\\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\\njava.base/java.lang.reflect.Method.invoke(Unknown Source)\\norg.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\\norg.apache...\"}\"\n>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Join operation will definitely show distributed activity\u001b[39;00m\n\u001b[32m      6\u001b[39m joined = df1.join(df2, \u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minner\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m result = \u001b[43mjoined\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mJoined count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/dataframe.py:310\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    308\u001b[39m     table, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_invoke_function\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcount\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_to_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m table[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].as_py()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/dataframe.py:1791\u001b[39m, in \u001b[36mDataFrame._to_table\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_to_table\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tuple[\u001b[33m\"\u001b[39m\u001b[33mpa.Table\u001b[39m\u001b[33m\"\u001b[39m, Optional[StructType]]:\n\u001b[32m   1790\u001b[39m     query = \u001b[38;5;28mself\u001b[39m._plan.to_proto(\u001b[38;5;28mself\u001b[39m._session.client)\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m     table, schema, \u001b[38;5;28mself\u001b[39m._execution_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plan\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobservations\u001b[49m\n\u001b[32m   1793\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1794\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1795\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (table, schema)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/client/core.py:925\u001b[39m, in \u001b[36mSparkConnectClient.to_table\u001b[39m\u001b[34m(self, plan, observations)\u001b[39m\n\u001b[32m    923\u001b[39m req = \u001b[38;5;28mself\u001b[39m._execute_plan_request_with_metadata()\n\u001b[32m    924\u001b[39m req.plan.CopyFrom(plan)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m table, schema, metrics, observed_metrics, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n\u001b[32m    928\u001b[39m ei = ExecutionInfo(metrics, observed_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/client/core.py:1560\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch\u001b[39m\u001b[34m(self, req, observations, self_destruct)\u001b[39m\n\u001b[32m   1557\u001b[39m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers=\u001b[38;5;28mself\u001b[39m._progress_handlers, operation_id=req.operation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch_as_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1563\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/client/core.py:1537\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req, observations, progress)\u001b[39m\n\u001b[32m   1535\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[32m   1536\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m1537\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/client/core.py:1811\u001b[39m, in \u001b[36mSparkConnectClient._handle_error\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m   1809\u001b[39m     \u001b[38;5;28mself\u001b[39m.thread_local.inside_error_handling = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1810\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc.RpcError):\n\u001b[32m-> \u001b[39m\u001b[32m1811\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m   1813\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/client/core.py:1886\u001b[39m, in \u001b[36mSparkConnectClient._handle_rpc_error\u001b[39m\u001b[34m(self, rpc_error)\u001b[39m\n\u001b[32m   1879\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m info.metadata[\u001b[33m\"\u001b[39m\u001b[33merrorClass\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mINVALID_HANDLE.SESSION_CHANGED\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1880\u001b[39m                 \u001b[38;5;28mself\u001b[39m._closed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1882\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[32m   1883\u001b[39m                 info,\n\u001b[32m   1884\u001b[39m                 status.message,\n\u001b[32m   1885\u001b[39m                 \u001b[38;5;28mself\u001b[39m._fetch_enriched_error(info),\n\u001b[32m-> \u001b[39m\u001b[32m1886\u001b[39m                 \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_display_server_stack_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1887\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1889\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status.message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/client/core.py:1840\u001b[39m, in \u001b[36mSparkConnectClient._display_server_stack_trace\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1838\u001b[39m conf = RuntimeConf(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1839\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1840\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mconf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.connect.serverStacktrace.enabled\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m == \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1841\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1842\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m conf.get(\u001b[33m\"\u001b[39m\u001b[33mspark.sql.pyspark.jvmStacktrace.enabled\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/conf.py:80\u001b[39m, in \u001b[36mRuntimeConf.get\u001b[39m\u001b[34m(self, key, default)\u001b[39m\n\u001b[32m     76\u001b[39m     op_get_with_default = proto.ConfigRequest.GetWithDefault(\n\u001b[32m     77\u001b[39m         pairs=[proto.KeyValue(key=key, value=cast(Optional[\u001b[38;5;28mstr\u001b[39m], default))]\n\u001b[32m     78\u001b[39m     )\n\u001b[32m     79\u001b[39m     operation = proto.ConfigRequest.Operation(get_with_default=op_get_with_default)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result.pairs[\u001b[32m0\u001b[39m][\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/client/core.py:1655\u001b[39m, in \u001b[36mSparkConnectClient.config\u001b[39m\u001b[34m(self, operation)\u001b[39m\n\u001b[32m   1653\u001b[39m req.operation.CopyFrom(operation)\n\u001b[32m   1654\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1655\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retrying\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1656\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1657\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_builder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/client/retries.py:251\u001b[39m, in \u001b[36mRetrying.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m AttemptManager(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._done:\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m AttemptManager(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark-cluster/.venv/lib/python3.13/site-packages/pyspark/sql/connect/client/retries.py:231\u001b[39m, in \u001b[36mRetrying._wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wait_time \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    226\u001b[39m         logger.debug(\n\u001b[32m    227\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(exception)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    228\u001b[39m             + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWill retry after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ms (policy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolicy.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    229\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# Exceeded retries\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create two DataFrames and join them\n",
    "df1 = spark.range(0, 100000000, numPartitions=4).withColumnRenamed(\"id\", \"key\")\n",
    "df2 = spark.range(500000, 150000000, numPartitions=4).withColumnRenamed(\"id\", \"key\")\n",
    "\n",
    "# Join operation will definitely show distributed activity\n",
    "joined = df1.join(df2, \"key\", \"inner\")\n",
    "result = joined.count()\n",
    "print(f\"Joined count: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
